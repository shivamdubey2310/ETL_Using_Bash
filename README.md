# ETL Pipeline Using Bash & Python

![ETL Pipeline](https://img.shields.io/badge/ETL-Pipeline-blue)
![Bash](https://img.shields.io/badge/Bash-4EAA25?logo=gnu-bash&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?logo=postgresql&logoColor=white)

A robust ETL (Extract, Transform, Load) pipeline that extracts user data from the RandomUser.me API, processes it using Python/Pandas, and loads it into a PostgreSQL database with automated scheduling via cron jobs.

## ğŸš€ Features

- **Data Extraction**: Automated data retrieval from [RandomUser.me API](https://randomuser.me/api/) using Bash/curl
- **Data Transformation**: Efficient data processing with Python and Pandas
- **Database Loading**: Structured data storage in PostgreSQL using SQLAlchemy
- **Automated Scheduling**: Cron job automation for daily pipeline execution
- **Comprehensive Logging**: Full pipeline logging for monitoring and debugging
- **Normalized Database Schema**: Properly structured relational database design

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RandomUser    â”‚    â”‚   Python        â”‚    â”‚   PostgreSQL    â”‚
â”‚   API           â”‚â”€â”€â–¶â”‚   Transform     â”‚â”€â”€â–¶â”‚   Database      â”‚
â”‚   (Source)      â”‚    â”‚   (Process)     â”‚    â”‚   (Target)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                        â–²                     â–²
        â”‚                        â”‚                     â”‚
   Bash/curl              Pandas/SQLAlchemy      Normalized Schema
```

## ğŸ“š Prerequisites

Before running this ETL pipeline, ensure you have:

- **Linux/Unix environment** (tested on Linux)
- **Bash shell** (version 4.0+)
- **Python 3.7+** with pip
- **PostgreSQL** database server
- **curl** command-line tool
- **crontab** for scheduling (optional)

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/ETL_Using_Bash.git
   cd ETL_Using_Bash
   ```

2. **Install Python dependencies**:
   ```bash
   pip3 install -r requirements.txt
   ```

3. **Create PostgreSQL database**:
   ```sql
   CREATE DATABASE your_database_name;
   ```

4. **Configure environment variables**:
   Create a `.env` file in the project root:
   ```env
   DB_HOST=localhost
   DB_PORT=5432
   DB_NAME=your_database_name
   DB_USER=your_username
   DB_PASSWORD=your_password
   ```

5. **Make scripts executable**:
   ```bash
   chmod +x Main.sh BashScripts/*.sh
   ```

## ğŸš€ Usage

### Manual Execution

Run the complete ETL pipeline:
```bash
./Main.sh
```

### Individual Components

- **Extract data only**:
  ```bash
  ./BashScripts/Extraction.sh
  ```

- **Transform data only**:
  ```bash
  python3 PythonScripts/Transform.py
  ```

- **Create database tables**:
  ```bash
  python3 PythonScripts/CreateTable.py
  ```

- **Load data only**:
  ```bash
  ./BashScripts/Load.sh
  ```

## ğŸ—„ï¸ Database Schema

The pipeline creates a normalized database schema with the following tables:

- **person** - Main user information table
- **name** - User names (first, last, title)
- **location** - Address and location data
- **login** - Authentication details
- **dob** - Date of birth information
- **registered** - Registration timestamps
- **picture** - Profile picture URLs

> ğŸ“„ See `DB_Schema.txt` for detailed schema specifications.

## â° Scheduling

To automate the pipeline execution:

1. **Open crontab editor**:
   ```bash
   crontab -e
   ```

2. **Add the following line** to run daily at 2:00 AM:
   ```bash
   0 2 * * * /path/to/your/project/Main.sh >> /path/to/your/project/logs/cron.log 2>&1
   ```

3. **Verify cron job**:
   ```bash
   crontab -l
   ```

## ğŸ“ Project Structure

```
ETL_Using_Bash/
â”œâ”€â”€ Main.sh                 # Main pipeline orchestrator
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ DB_Schema.txt         # Database schema documentation
â”œâ”€â”€ .env                  # Environment variables (create this)
â”œâ”€â”€ BashScripts/
â”‚   â”œâ”€â”€ Extraction.sh     # Data extraction script
â”‚   â””â”€â”€ Load.sh          # Data loading script
â”œâ”€â”€ PythonScripts/
â”‚   â”œâ”€â”€ Transform.py     # Data transformation logic
â”‚   â””â”€â”€ CreateTable.py   # Database table creation
â”œâ”€â”€ data/                # Raw and processed data files
â”‚   â”œâ”€â”€ raw_data.csv    # Original extracted data
â”‚   â””â”€â”€ *_raw.csv       # Transformed data files
â””â”€â”€ logs/               # Pipeline execution logs
    â”œâ”€â”€ pipeline.log    # Main pipeline logs
    â””â”€â”€ cron.log       # Cron job execution logs
```

## ğŸ“Š Logs

The pipeline generates comprehensive logs:

- **`logs/pipeline.log`**: Main pipeline execution logs with timestamps
- **`logs/cron.log`**: Cron job execution logs and errors

Monitor logs in real-time:
```bash
tail -f logs/pipeline.log
```

## âš ï¸ Important Notes

- **Database Creation**: You must manually create the PostgreSQL database before running the pipeline
- **No Virtual Environment**: This project is designed to run without Python virtual environments
- **Terminal Restart**: If the pipeline doesn't run initially, try restarting your terminal
- **File Permissions**: Ensure all shell scripts have execute permissions

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
---

**Built with â¤ï¸ using Bash, Python, and PostgreSQL**